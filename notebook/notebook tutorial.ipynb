{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfxxR9NG8rmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Header Section\n",
        "\n",
        "## Assignment Title: Specialized Wikipedia Research Assistant\n",
        "\n",
        "## Research Domain: Techonology - Artificial Intelligence\n",
        "The selected technology domain is subfield of Artifical Intelligence. This field is chosen because of many of these topics are covered during my undergradaute and my current graduate studies, however during my udergraduate many of these topics are thought by themselves. For example, state search algorthims, while interesting and helped me solve games like tower of hanoi, backgammon, are not that connected to things like traditional machine learning, deep learning etc. I found it difficult to link these topics during my undergraduate and after awhile, forgot about these concept. This research assistance may be able to help breach this gap.\n",
        "\n",
        "## Link of 5 Wikipedia websites\n",
        "https://en.wikipedia.org/wiki/Agentic_AI <br>\n",
        "https://en.wikipedia.org/wiki/Reinforcement_learning <br>\n",
        "https://en.wikipedia.org/wiki/Machine_learning <br>\n",
        "https://en.wikipedia.org/wiki/Deep_learning <br>\n",
        "https://en.wikipedia.org/wiki/Natural_language_processing <br>\n"
      ],
      "metadata": {
        "id": "dSbXc34jxOhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Setup and Configuration\n",
        "Install neccessary packages and set up API (OpenAI)"
      ],
      "metadata": {
        "id": "VEi8yTA08uWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrOqpXzH8NTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a7f6c3-8dbb-4a7d-8bf6-f37f172b9995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install crawl4ai>=0.2.0 openai>=1.0.0 pydantic>=2.0.0 python-dotenv>=1.0.0 requests>=2.25.0 beautifulsoup4>=4.9.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!playwright install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdzs2xUtUCoK",
        "outputId": "ee04a72c-7169-4c2e-f76a-cc2d3152bc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Chromium 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G173.7 MiB [] 0% 132.2s\u001b[0K\u001b[1G173.7 MiB [] 0% 19.3s\u001b[0K\u001b[1G173.7 MiB [] 0% 6.2s\u001b[0K\u001b[1G173.7 MiB [] 1% 4.8s\u001b[0K\u001b[1G173.7 MiB [] 1% 4.4s\u001b[0K\u001b[1G173.7 MiB [] 1% 4.3s\u001b[0K\u001b[1G173.7 MiB [] 2% 4.0s\u001b[0K\u001b[1G173.7 MiB [] 2% 3.8s\u001b[0K\u001b[1G173.7 MiB [] 3% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 3% 3.7s\u001b[0K\u001b[1G173.7 MiB [] 4% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 4% 3.5s\u001b[0K\u001b[1G173.7 MiB [] 5% 3.5s\u001b[0K\u001b[1G173.7 MiB [] 5% 4.1s\u001b[0K\u001b[1G173.7 MiB [] 6% 3.9s\u001b[0K\u001b[1G173.7 MiB [] 7% 3.7s\u001b[0K\u001b[1G173.7 MiB [] 8% 3.4s\u001b[0K\u001b[1G173.7 MiB [] 9% 3.3s\u001b[0K\u001b[1G173.7 MiB [] 9% 3.2s\u001b[0K\u001b[1G173.7 MiB [] 10% 3.0s\u001b[0K\u001b[1G173.7 MiB [] 11% 2.9s\u001b[0K\u001b[1G173.7 MiB [] 12% 2.7s\u001b[0K\u001b[1G173.7 MiB [] 13% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 14% 2.5s\u001b[0K\u001b[1G173.7 MiB [] 15% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 16% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 17% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 18% 2.2s\u001b[0K\u001b[1G173.7 MiB [] 19% 2.1s\u001b[0K\u001b[1G173.7 MiB [] 20% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 21% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 22% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 23% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 24% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 25% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 26% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 27% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 28% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 29% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 30% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 31% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 32% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 33% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 34% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 35% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 36% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 37% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 38% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 39% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 40% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 41% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 42% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 43% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 44% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 45% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 46% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 47% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 48% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 50% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 51% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 52% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 53% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 53% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 54% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 55% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 56% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 57% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 58% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 58% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 59% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 60% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 61% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 61% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 63% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 64% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 65% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 66% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 67% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 70% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 71% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 72% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 74% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 75% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 76% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 78% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 79% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 79% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 81% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 82% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 83% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 86% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 90% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 94% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium-1187\n",
            "Downloading Chromium Headless Shell 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 0% 42.9s\u001b[0K\u001b[1G104.3 MiB [] 0% 6.3s\u001b[0K\u001b[1G104.3 MiB [] 1% 3.1s\u001b[0K\u001b[1G104.3 MiB [] 2% 2.9s\u001b[0K\u001b[1G104.3 MiB [] 2% 2.8s\u001b[0K\u001b[1G104.3 MiB [] 3% 3.0s\u001b[0K\u001b[1G104.3 MiB [] 3% 2.9s\u001b[0K\u001b[1G104.3 MiB [] 5% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 6% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 7% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 8% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 9% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 9% 2.3s\u001b[0K\u001b[1G104.3 MiB [] 9% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 9% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 9% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 10% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 11% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 12% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 13% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 14% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 15% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 15% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 16% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 17% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 18% 2.3s\u001b[0K\u001b[1G104.3 MiB [] 20% 2.2s\u001b[0K\u001b[1G104.3 MiB [] 21% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 22% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 23% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 24% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 25% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 26% 1.8s\u001b[0K\u001b[1G104.3 MiB [] 27% 1.8s\u001b[0K\u001b[1G104.3 MiB [] 28% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 30% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 31% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 32% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 33% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 34% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 35% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 36% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 37% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 38% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 39% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 40% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 41% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 43% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 44% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 45% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 46% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 47% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 48% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 48% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 50% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 51% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 52% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 53% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 54% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 56% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 58% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 59% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 60% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 62% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 63% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 64% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 65% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 66% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 67% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 68% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 69% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 70% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 72% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 73% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 74% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 76% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 77% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 79% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 81% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 82% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 84% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 85% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 86% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 91% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 97% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1187\n",
            "Downloading Firefox 141.0 (playwright build v1490)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1490/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G96 MiB [] 0% 0.0s\u001b[0K\u001b[1G96 MiB [] 0% 8.5s\u001b[0K\u001b[1G96 MiB [] 0% 13.7s\u001b[0K\u001b[1G96 MiB [] 0% 17.2s\u001b[0K\u001b[1G96 MiB [] 0% 20.2s\u001b[0K\u001b[1G96 MiB [] 0% 22.2s\u001b[0K\u001b[1G96 MiB [] 0% 23.8s\u001b[0K\u001b[1G96 MiB [] 0% 25.1s\u001b[0K\u001b[1G96 MiB [] 0% 26.2s\u001b[0K\u001b[1G96 MiB [] 0% 27.1s\u001b[0K\u001b[1G96 MiB [] 1% 28.0s\u001b[0K\u001b[1G96 MiB [] 1% 29.6s\u001b[0K\u001b[1G96 MiB [] 1% 31.3s\u001b[0K\u001b[1G96 MiB [] 1% 32.6s\u001b[0K\u001b[1G96 MiB [] 1% 33.7s\u001b[0K\u001b[1G96 MiB [] 1% 34.8s\u001b[0K\u001b[1G96 MiB [] 1% 35.7s\u001b[0K\u001b[1G96 MiB [] 1% 36.5s\u001b[0K\u001b[1G96 MiB [] 1% 37.3s\u001b[0K\u001b[1G96 MiB [] 1% 38.1s\u001b[0K\u001b[1G96 MiB [] 1% 38.7s\u001b[0K\u001b[1G96 MiB [] 1% 39.4s\u001b[0K\u001b[1G96 MiB [] 1% 39.9s\u001b[0K\u001b[1G96 MiB [] 1% 40.6s\u001b[0K\u001b[1G96 MiB [] 1% 41.1s\u001b[0K\u001b[1G96 MiB [] 1% 41.5s\u001b[0K\u001b[1G96 MiB [] 2% 42.0s\u001b[0K\u001b[1G96 MiB [] 2% 42.3s\u001b[0K\u001b[1G96 MiB [] 2% 42.6s\u001b[0K\u001b[1G96 MiB [] 2% 43.0s\u001b[0K\u001b[1G96 MiB [] 2% 43.4s\u001b[0K\u001b[1G96 MiB [] 2% 43.7s\u001b[0K\u001b[1G96 MiB [] 2% 44.1s\u001b[0K\u001b[1G96 MiB [] 2% 44.9s\u001b[0K\u001b[1G96 MiB [] 2% 45.2s\u001b[0K\u001b[1G96 MiB [] 2% 45.4s\u001b[0K\u001b[1G96 MiB [] 2% 48.4s\u001b[0K\u001b[1G96 MiB [] 2% 48.5s\u001b[0K\u001b[1G96 MiB [] 2% 48.6s\u001b[0K\u001b[1G96 MiB [] 2% 48.7s\u001b[0K\u001b[1G96 MiB [] 3% 48.9s\u001b[0K\u001b[1G96 MiB [] 3% 49.7s\u001b[0K\u001b[1G96 MiB [] 3% 49.8s\u001b[0K\u001b[1G96 MiB [] 3% 50.0s\u001b[0K\u001b[1G96 MiB [] 3% 49.9s\u001b[0K\u001b[1G96 MiB [] 3% 50.0s\u001b[0K\u001b[1G96 MiB [] 4% 36.6s\u001b[0K\u001b[1G96 MiB [] 6% 27.1s\u001b[0K\u001b[1G96 MiB [] 8% 21.1s\u001b[0K\u001b[1G96 MiB [] 9% 18.2s\u001b[0K\u001b[1G96 MiB [] 11% 15.7s\u001b[0K\u001b[1G96 MiB [] 12% 14.2s\u001b[0K\u001b[1G96 MiB [] 13% 12.4s\u001b[0K\u001b[1G96 MiB [] 15% 10.7s\u001b[0K\u001b[1G96 MiB [] 17% 10.0s\u001b[0K\u001b[1G96 MiB [] 18% 9.1s\u001b[0K\u001b[1G96 MiB [] 20% 8.0s\u001b[0K\u001b[1G96 MiB [] 22% 7.2s\u001b[0K\u001b[1G96 MiB [] 24% 6.7s\u001b[0K\u001b[1G96 MiB [] 25% 6.1s\u001b[0K\u001b[1G96 MiB [] 27% 5.6s\u001b[0K\u001b[1G96 MiB [] 29% 5.3s\u001b[0K\u001b[1G96 MiB [] 31% 4.9s\u001b[0K\u001b[1G96 MiB [] 33% 4.5s\u001b[0K\u001b[1G96 MiB [] 35% 4.0s\u001b[0K\u001b[1G96 MiB [] 37% 3.7s\u001b[0K\u001b[1G96 MiB [] 39% 3.5s\u001b[0K\u001b[1G96 MiB [] 41% 3.2s\u001b[0K\u001b[1G96 MiB [] 43% 3.0s\u001b[0K\u001b[1G96 MiB [] 45% 2.8s\u001b[0K\u001b[1G96 MiB [] 47% 2.5s\u001b[0K\u001b[1G96 MiB [] 48% 2.5s\u001b[0K\u001b[1G96 MiB [] 49% 2.4s\u001b[0K\u001b[1G96 MiB [] 52% 2.2s\u001b[0K\u001b[1G96 MiB [] 54% 2.0s\u001b[0K\u001b[1G96 MiB [] 56% 1.8s\u001b[0K\u001b[1G96 MiB [] 59% 1.7s\u001b[0K\u001b[1G96 MiB [] 61% 1.5s\u001b[0K\u001b[1G96 MiB [] 62% 1.5s\u001b[0K\u001b[1G96 MiB [] 68% 1.2s\u001b[0K\u001b[1G96 MiB [] 70% 1.1s\u001b[0K\u001b[1G96 MiB [] 72% 1.0s\u001b[0K\u001b[1G96 MiB [] 74% 0.8s\u001b[0K\u001b[1G96 MiB [] 76% 0.8s\u001b[0K\u001b[1G96 MiB [] 78% 0.7s\u001b[0K\u001b[1G96 MiB [] 79% 0.7s\u001b[0K\u001b[1G96 MiB [] 80% 0.6s\u001b[0K\u001b[1G96 MiB [] 82% 0.6s\u001b[0K\u001b[1G96 MiB [] 84% 0.5s\u001b[0K\u001b[1G96 MiB [] 85% 0.5s\u001b[0K\u001b[1G96 MiB [] 87% 0.4s\u001b[0K\u001b[1G96 MiB [] 88% 0.3s\u001b[0K\u001b[1G96 MiB [] 91% 0.3s\u001b[0K\u001b[1G96 MiB [] 93% 0.2s\u001b[0K\u001b[1G96 MiB [] 95% 0.1s\u001b[0K\u001b[1G96 MiB [] 97% 0.1s\u001b[0K\u001b[1G96 MiB [] 99% 0.0s\u001b[0K\u001b[1G96 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 141.0 (playwright build v1490) downloaded to /root/.cache/ms-playwright/firefox-1490\n",
            "Downloading Webkit 26.0 (playwright build v2203)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2203/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G94.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G94.6 MiB [] 0% 2.7s\u001b[0K\u001b[1G94.6 MiB [] 1% 2.0s\u001b[0K\u001b[1G94.6 MiB [] 2% 1.6s\u001b[0K\u001b[1G94.6 MiB [] 3% 1.6s\u001b[0K\u001b[1G94.6 MiB [] 4% 1.6s\u001b[0K\u001b[1G94.6 MiB [] 6% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 7% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 8% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 9% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 10% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 11% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 12% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 13% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 14% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 16% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 17% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 18% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 19% 1.3s\u001b[0K\u001b[1G94.6 MiB [] 21% 1.3s\u001b[0K\u001b[1G94.6 MiB [] 22% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 24% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 25% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 26% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 27% 1.1s\u001b[0K\u001b[1G94.6 MiB [] 28% 1.1s\u001b[0K\u001b[1G94.6 MiB [] 30% 1.1s\u001b[0K\u001b[1G94.6 MiB [] 32% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 33% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 34% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 36% 0.9s\u001b[0K\u001b[1G94.6 MiB [] 37% 0.9s\u001b[0K\u001b[1G94.6 MiB [] 39% 0.9s\u001b[0K\u001b[1G94.6 MiB [] 41% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 42% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 44% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 47% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 48% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 49% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 52% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 55% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 57% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 60% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 62% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 65% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 67% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 70% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 73% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 76% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 78% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 81% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 86% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 88% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G94.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 26.0 (playwright build v2203) downloaded to /root/.cache/ms-playwright/webkit-2203\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 57% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:934:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1056:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1045:7)\n",
            "    at async i.<anonymous> (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and set up openAI"
      ],
      "metadata": {
        "id": "ACzE7f9dUE07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import getpass\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "\n",
        "# Crawl4AI imports\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n",
        "from crawl4ai.chunking_strategy import RegexChunking, SlidingWindowChunking\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "from crawl4ai import CacheMode\n",
        "\n",
        "# LLM and embedding imports\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MH0zC-BwTRw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API handling"
      ],
      "metadata": {
        "id": "tuIm_sT84otT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - Update with your API keys\n",
        "api_key = getpass.getpass(prompt=\"Enter your OpenAI API key (it will not be shown as you type): \")\n",
        "# Set the environment variable with the key\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "def create_secure_openai_client():\n",
        "    \"\"\"\n",
        "    Create OpenAI client with secure API key handling.\n",
        "\n",
        "    This function:\n",
        "    1. Looks for OPENAI_API_KEY in environment variables\n",
        "    2. Tests the connection with a simple API call\n",
        "    3. Returns the client or None if setup fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()  # Load .env file if it exists\n",
        "    except ImportError:\n",
        "        pass  # python-dotenv not installed, that's okay\n",
        "\n",
        "    api_key = os.getenv('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        print(\"⚠️ No OpenAI API key found.\")\n",
        "        print(\"💡 Set environment variable: OPENAI_API_KEY=your_key\")\n",
        "        print(\"💡 Or create .env file with: OPENAI_API_KEY=your_key\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        # Test connection with a simple API call\n",
        "        models = client.models.list()\n",
        "        print(\"✅ OpenAI client created and tested successfully\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OpenAI client creation failed: {e}\")\n",
        "        print(\"🔍 Check your API key and internet connection\")\n",
        "        return None\n",
        "\n",
        "# Initialize the client\n",
        "client = create_secure_openai_client()"
      ],
      "metadata": {
        "id": "q7W5s0B0qhI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44372338-d077-48eb-b5b9-26fe15fd6381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key (it will not be shown as you type): ··········\n",
            "✅ OpenAI client created and tested successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Wikipedia Scraper Implementation\n",
        "The following below is the WikipediaScraper class, it has a list of urls to scrape. Some cleaning based on Wikipedia's raw markdown format is done in the clean_content method. In this case, the 5 URL are mentioned above\n",
        "## Link of 5 Wikipedia websites\n",
        "https://en.wikipedia.org/wiki/Agentic_AI <br>\n",
        "https://en.wikipedia.org/wiki/Reinforcement_learning <br>\n",
        "https://en.wikipedia.org/wiki/Machine_learning <br>\n",
        "https://en.wikipedia.org/wiki/Deep_learning <br>\n",
        "https://en.wikipedia.org/wiki/Natural_language_processing <br>\n",
        "\n",
        "## Explanation of how articles relate to research domain\n",
        "All aritcles are subclasses of Artifical Intelligence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xxryyoOazHXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WikipediaScraper:\n",
        "  def __init__(self, base_urls: List[str]):\n",
        "    self.base_urls = base_urls\n",
        "\n",
        "  async def scrape_article(self, url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Scrape a single Wikipedia article\"\"\"\n",
        "\n",
        "    # Configure optimized crawling\n",
        "    chunking_strategy = SlidingWindowChunking(\n",
        "        window_size=1000,    # Optimal for LLM processing\n",
        "        step=500            # 50% overlap for context preservation\n",
        "    )\n",
        "\n",
        "    config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator(),\n",
        "        chunking_strategy=chunking_strategy,\n",
        "        cache_mode=CacheMode.WRITE_ONLY,  # results of this crawl are saved to the cache\n",
        "        page_timeout=15000,  # Faster timeout for efficiency\n",
        "        verbose=False       # Reduce logging for speed\n",
        "    )\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "      try:\n",
        "        result = await crawler.arun(url=url, config=config)\n",
        "        if result.success:\n",
        "          raw_content = result.markdown.raw_markdown\n",
        "          cleaned_content = self.clean_content(raw_content)\n",
        "          return {\n",
        "              \"url\": url,\n",
        "              \"title\": result.metadata.get(\"title\", \"Unknown\"),\n",
        "              \"raw_content\": cleaned_content,\n",
        "              \"success\": True\n",
        "          }\n",
        "        else:\n",
        "          print(f\"   ❌ Failed: {result.error_message}\")\n",
        "          return {\n",
        "              \"url\": url,\n",
        "              \"title\": \"Unknown\",\n",
        "              \"raw_content\": result.error_message,\n",
        "              \"success\": False,\n",
        "          }\n",
        "      except Exception as e:\n",
        "        print(f\"   ❌ Exception: {e}\")\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": \"Unknown\",\n",
        "            \"raw_content\": str(e),\n",
        "            \"success\": False,\n",
        "        }\n",
        "\n",
        "\n",
        "  async def scrape_multiple(self) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Scrape all articles in the URL list\"\"\"\n",
        "    results =[]\n",
        "    start_time= time.time()\n",
        "    for url in self.base_urls:\n",
        "      print(f\"⚡ Scraping: {url}\")\n",
        "      article_result = await self.scrape_article(url)\n",
        "      results.append(article_result)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"✅ Finished scraping {len(self.base_urls)} articles in {elapsed:.2f} seconds\")\n",
        "\n",
        "    return results\n",
        "\n",
        "  def clean_content(self, raw_content: str) -> str:\n",
        "    \"\"\"Clean and preprocess scraped content\"\"\"\n",
        "    text = raw_content\n",
        "\n",
        "    # 1. Keep only from the first H1 (# Title) onwards\n",
        "    match = re.search(r\"^# .+\", text, flags=re.MULTILINE)\n",
        "    if match:\n",
        "        text = text[match.start():]\n",
        "\n",
        "    # 2. Remove navigation / sidebar / account / tools sections by keyword\n",
        "    remove_keywords = [\n",
        "        \"Main menu\", \"Navigation\", \"Contribute\", \"Appearance\", \"Personal tools\",\n",
        "        \"Pages for logged out editors\", \"Toggle the table of contents\",\n",
        "        \"Tools\", \"Print/export\", \"In other projects\",\n",
        "        \"Edit links\", \"Create account\", \"Log in\", \"Donate\",\n",
        "        \"move to sidebar hide\", \"Search\", \"Width\", \"Color (beta)\", \"Actions\",\n",
        "    ]\n",
        "    pattern = re.compile(rf\"^.*({'|'.join(map(re.escape, remove_keywords))}).*$\",\n",
        "                         re.MULTILINE | re.IGNORECASE)\n",
        "    text = pattern.sub(\"\", text)\n",
        "\n",
        "    # 3. Remove [edit] links\n",
        "    text = re.sub(r\"\\[\\[edit.*?\\]\\]\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "\n",
        "    # 4. Remove inline citation tooltips (#cite_note stuff)\n",
        "    text = re.sub(r\"\\[\\d+\\]\\(https:\\/\\/en\\.wikipedia\\.org\\/wiki\\/[^)]+#cite_note[^\\)]*\\)\",\n",
        "                  lambda m: \"[\" + re.search(r\"\\d+\", m.group()).group() + \"]\",\n",
        "                  text)\n",
        "\n",
        "    # 5. Remove excess blank lines\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
        "\n",
        "    return text.strip()\n",
        "    # chunks = raw_content.split('\\n\\n')\n",
        "\n",
        "    # # Quality filter: only keep substantial chunksq\n",
        "    # quality_chunks = [\n",
        "    #     chunk for chunk in chunks\n",
        "    #     if len(chunk.strip()) > 100 and\n",
        "    #     not chunk.strip().startswith(('cookie', 'javascript', 'advertisement', 'navigation', 'contribute'))\n",
        "    # ]\n",
        "    # # Filter out junk lines\n",
        "    # def regex_clean(text:str) -> str:\n",
        "    #   text = re.sub(r\"\\[edit\\]\", \"\", text)\n",
        "    #   text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
        "    #   text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n",
        "    #   text = re.sub(r\"Category:.*\", \"\", text)\n",
        "    #   text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
        "    #   return text.strip()\n",
        "\n",
        "    # cleaned_quality_chunks = [regex_clean(c) for c in quality_chunks]\n",
        "\n",
        "    # return quality_chunks"
      ],
      "metadata": {
        "id": "dww4eGNLahpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration of Wikipedia Scraper\n",
        "Below is a simple demonstation, the 5 links are used to create this scraper class, and results are saved into scrapper_result, which is a list of dictionary and each dictionary has 4 fields\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  \"url\": url,\n",
        "  \"title\": result.metadata.get(\"title\", \"Unknown\"),\n",
        "  \"raw_content\": cleaned_content,\n",
        "  \"success\": True\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pbZBreW18xkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_scraper = WikipediaScraper([\"https://en.wikipedia.org/wiki/Agentic_AI\", \"https://en.wikipedia.org/wiki/Reinforcement_learning\", \"https://en.wikipedia.org/wiki/Machine_learning\", \"https://en.wikipedia.org/wiki/Deep_learning\", \"https://en.wikipedia.org/wiki/Natural_language_processing\"])\n",
        "scrapper_result = await ai_scraper.scrape_multiple()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "sPNikujTd_Ib",
        "outputId": "99a7abd1-7de3-4617-fc15-d2ed8304b768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: https://en.wikipedia.org/wiki/Agentic_AI\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mCOMPLETE\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m ● Database backup created at: \u001b[0m\u001b[36m/root/.crawl4ai/\u001b[0m\u001b[36mcrawl4ai.db.backup_20250901_061440\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">COMPLETE</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\"> ● Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20250901_061440 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Starting database migration\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Starting database migration... </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● Migration completed. \u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m records processed. \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● Migration completed. </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> records processed. </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: https://en.wikipedia.org/wiki/Reinforcement_learning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: https://en.wikipedia.org/wiki/Machine_learning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: https://en.wikipedia.org/wiki/Deep_learning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: https://en.wikipedia.org/wiki/Natural_language_processing\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished scraping 5 articles in 21.62 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in scrapper_result[0].items():\n",
        "  if key != \"raw_content\":\n",
        "    print(f\"{key} : {value}\")\n",
        "  else:\n",
        "    print(f\"{key} : {value[:2000] + \"...\"}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9hyjhgz5PRg",
        "outputId": "37796724-b688-40ff-8cf6-78073712cf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url : https://en.wikipedia.org/wiki/Agentic_AI\n",
            "title : Agentic AI - Wikipedia\n",
            "raw_content : # Agentic AI\n",
            "4 languages\n",
            "  * [Català](https://ca.wikipedia.org/wiki/IA_ag%C3%A8ntica \"IA agèntica – Catalan\")\n",
            "  * [فارسی](https://fa.wikipedia.org/wiki/%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C_%D8%AE%D9%88%D8%AF%D9%85%D8%AE%D8%AA%D8%A7%D8%B1 \"هوش مصنوعی خودمختار – Persian\")\n",
            "  * [한국어](https://ko.wikipedia.org/wiki/%EC%97%90%EC%9D%B4%EC%A0%84%ED%8B%B1_AI \"에이전틱 AI – Korean\")\n",
            "  * [Русский](https://ru.wikipedia.org/wiki/%D0%90%D0%B3%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D0%98%D0%98 \"Агентный ИИ – Russian\")\n",
            "\n",
            "  * [Article](https://en.wikipedia.org/wiki/Agentic_AI \"View the content page \\[alt-c\\]\")\n",
            "  * [Talk](https://en.wikipedia.org/wiki/Talk:Agentic_AI \"Discuss improvements to the content page \\[alt-t\\]\")\n",
            "\n",
            "English\n",
            "  * [Read](https://en.wikipedia.org/wiki/Agentic_AI)\n",
            "  * [Edit](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=edit \"Edit this page \\[alt-e\\]\")\n",
            "  * [View history](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=history \"Past revisions of this page \\[alt-h\\]\")\n",
            "\n",
            "  * [Read](https://en.wikipedia.org/wiki/Agentic_AI)\n",
            "  * [Edit](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=edit \"Edit this page \\[alt-e\\]\")\n",
            "  * [View history](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=history)\n",
            "\n",
            "General \n",
            "  * [What links here](https://en.wikipedia.org/wiki/Special:WhatLinksHere/Agentic_AI \"List of all English Wikipedia pages containing links to this page \\[alt-j\\]\")\n",
            "  * [Related changes](https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Agentic_AI \"Recent changes in pages linked from this page \\[alt-k\\]\")\n",
            "  * [Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard \"Upload files \\[alt-u\\]\")\n",
            "  * [Permanent link](https://en.wikipedia.org/w/index.php?title=Agentic_AI&oldid=1308638935 \"Permanent link to this revision of this page\")\n",
            "  * [Page information](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=info \"More information about this page\")\n",
            "  * [Cite this page](https://en.wikipedia...\n",
            "success : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of error handling\n",
        "Below is an example of error handling, in the list of URL, even though 1 of the website is a fake website, the correct one still gets scrapped"
      ],
      "metadata": {
        "id": "3EYhrvf9934F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Negative example and error handling for wrong URLs\n",
        "bad_scrapper = WikipediaScraper([\"notawebsite.com\",\"https://en.wikipedia.org/wiki/Agentic_AI\"])\n",
        "bad_scrapper_result = await bad_scrapper.scrape_multiple()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "drBPPPlB9fjV",
        "outputId": "44e73b49-1624-4f46-b3e8-133151bac699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Scraping: notawebsite.com\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ❌ Failed: Unexpected error in _crawl_web at line 500 in crawl (../usr/local/lib/python3.12/dist-packages/crawl4ai/async_crawler_strategy.py):\n",
            "Error: URL must start with 'http://', 'https://', 'file://', or 'raw:'\n",
            "\n",
            "Code context:\n",
            " 495                   status_code=status_code,\n",
            " 496                   screenshot=screenshot_data,\n",
            " 497                   get_delayed_content=None,\n",
            " 498               )\n",
            " 499           else:\n",
            " 500 →             raise ValueError(\n",
            " 501                   \"URL must start with 'http://', 'https://', 'file://', or 'raw:'\"\n",
            " 502               )\n",
            " 503   \n",
            " 504       async def _crawl_web(\n",
            " 505           self, url: str, config: CrawlerRunConfig\n",
            "⚡ Scraping: https://en.wikipedia.org/wiki/Agentic_AI\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished scraping 2 articles in 4.46 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_scrapper_result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sAG37rv9ypa",
        "outputId": "a22be3ca-e358-4d24-cb55-a4761cb33679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'url': 'notawebsite.com',\n",
              " 'title': 'Unknown',\n",
              " 'raw_content': 'Unexpected error in _crawl_web at line 500 in crawl (../usr/local/lib/python3.12/dist-packages/crawl4ai/async_crawler_strategy.py):\\nError: URL must start with \\'http://\\', \\'https://\\', \\'file://\\', or \\'raw:\\'\\n\\nCode context:\\n 495                   status_code=status_code,\\n 496                   screenshot=screenshot_data,\\n 497                   get_delayed_content=None,\\n 498               )\\n 499           else:\\n 500 →             raise ValueError(\\n 501                   \"URL must start with \\'http://\\', \\'https://\\', \\'file://\\', or \\'raw:\\'\"\\n 502               )\\n 503   \\n 504       async def _crawl_web(\\n 505           self, url: str, config: CrawlerRunConfig',\n",
              " 'success': False}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 Structured Data Extraction\n",
        "The following is the structured data extraction portion.\n",
        "## pydantic class\n",
        "First we create a pydantic class. To answer the required questions, 7 fields are chosen.\n",
        "\n",
        "\n",
        "```\n",
        "main_topic, summary, evolution_timeline, key_innovations, major_contributors, main_techniques, applications\n",
        "```\n"
      ],
      "metadata": {
        "id": "ssLM5cAn-S84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "class WikipediaExtraction(BaseModel):\n",
        "  \"\"\"Schema for artificial intelligence subfield\"\"\"\n",
        "  main_topic: str = Field(description=\"The name of the AI subfield (e.g., Machine Learning, Expert Systems, Computer Vision)\")\n",
        "  summary: str = Field(desciption=\"Concise overview of the subfield, its purpose, and scope\")\n",
        "  evolution_timeline: str = Field(description=\"Chronological evolution of this subfield, with notable milestones across time\")\n",
        "  key_innovations: str = Field(description=\"The most important breakthroughs, inventions, or discoveries that shaped this subfield\")\n",
        "  major_contributors: str = Field(description=\"Key researchers, organizations, or companies that significantly advanced this subfield\")\n",
        "  main_techniques: List[str] = Field(description=\"Core methods, models, or algorithms commonly used in this subfield\")\n",
        "  applications: List[str] = Field(description=\"Practical use cases and domains where this subfield has had major impact\")\n",
        "\n",
        "# Let's see what the JSON schema looks like\n",
        "print(\"📋 Generated JSON Schema Preview:\")\n",
        "print(json.dumps(WikipediaExtraction.model_json_schema(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsvf45fPgcvY",
        "outputId": "0d2752fa-5f1a-4ff5-a096-9d800d731647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Generated JSON Schema Preview:\n",
            "{\n",
            "  \"description\": \"Schema for artificial intelligence subfield\",\n",
            "  \"properties\": {\n",
            "    \"main_topic\": {\n",
            "      \"description\": \"The name of the AI subfield (e.g., Machine Learning, Expert Systems, Computer Vision)\",\n",
            "      \"title\": \"Main Topic\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"summary\": {\n",
            "      \"desciption\": \"Concise overview of the subfield, its purpose, and scope\",\n",
            "      \"title\": \"Summary\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"evolution_timeline\": {\n",
            "      \"description\": \"Chronological evolution of this subfield, with notable milestones across time\",\n",
            "      \"title\": \"Evolution Timeline\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"key_innovations\": {\n",
            "      \"description\": \"The most important breakthroughs, inventions, or discoveries that shaped this subfield\",\n",
            "      \"title\": \"Key Innovations\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"major_contributors\": {\n",
            "      \"description\": \"Key researchers, organizations, or companies that significantly advanced this subfield\",\n",
            "      \"title\": \"Major Contributors\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"main_techniques\": {\n",
            "      \"description\": \"Core methods, models, or algorithms commonly used in this subfield\",\n",
            "      \"items\": {\n",
            "        \"type\": \"string\"\n",
            "      },\n",
            "      \"title\": \"Main Techniques\",\n",
            "      \"type\": \"array\"\n",
            "    },\n",
            "    \"applications\": {\n",
            "      \"description\": \"Practical use cases and domains where this subfield has had major impact\",\n",
            "      \"items\": {\n",
            "        \"type\": \"string\"\n",
            "      },\n",
            "      \"title\": \"Applications\",\n",
            "      \"type\": \"array\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"main_topic\",\n",
            "    \"summary\",\n",
            "    \"evolution_timeline\",\n",
            "    \"key_innovations\",\n",
            "    \"major_contributors\",\n",
            "    \"main_techniques\",\n",
            "    \"applications\"\n",
            "  ],\n",
            "  \"title\": \"WikipediaExtraction\",\n",
            "  \"type\": \"object\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction class\n",
        "The following is the extraction class, cacheing and rate limiter are used for an optimized extraction."
      ],
      "metadata": {
        "id": "0j6p6fry_IBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedProcessor:\n",
        "    \"\"\"\n",
        "    Optimized processor for high-throughput applications.\n",
        "\n",
        "    Features:\n",
        "    - Batch processing for efficiency\n",
        "    - Async operations for concurrent requests\n",
        "    - Caching to reduce API calls\n",
        "    - Rate limiting to respect API limits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,client:Optional[OpenAI] = None):\n",
        "      self.client = client\n",
        "      self.cache = {}\n",
        "      self.rate_limiter = RateLimiter(requests_per_minute=60)\n",
        "\n",
        "\n",
        "\n",
        "    def extract_strucutred_data(self, content: str, model: str = \"gpt-4o-mini\") -> WikipediaExtraction:\n",
        "    #create schema for Open AI\n",
        "      schema = {\n",
        "          \"name\": \"AI_subfield_catalog\",\n",
        "          \"schema\": {\n",
        "              **WikipediaExtraction.model_json_schema(),\n",
        "              \"additionalProperties\": False\n",
        "          },\n",
        "          \"strict\": True  # This enforces strict schema compliance\n",
        "      }\n",
        "      try:\n",
        "        # The magic happens here - response_format enforces our schema\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=model,  # Only gpt-4o and gpt-4o-mini support structured outputs\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"Extract AI subfield information from text and structure it exactly according to the provided schema.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Extract and analyze AI subfield data from:\\n\\n{content}\"\n",
        "                }\n",
        "            ],\n",
        "            response_format={\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\": schema\n",
        "            }\n",
        "        )\n",
        "        # Direct validation - no parsing errors possible!\n",
        "        ai_subfield = WikipediaExtraction.model_validate_json(response.choices[0].message.content)\n",
        "\n",
        "        print(\"✅ Structured extraction successful!\")\n",
        "        return ai_subfield\n",
        "      except Exception as e:\n",
        "            print(f\"❌ Extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def batch_extract(self, articles: List[Dict]) -> List[WikipediaExtraction]:\n",
        "      final_dic= {}\n",
        "      for dic in articles:\n",
        "        current_title = dic[\"title\"]\n",
        "        # Check cache first\n",
        "        cache_key = hash(dic[\"raw_content\"][:100])\n",
        "        if cache_key in self.cache:\n",
        "          print(\"💾 Using cached result\")\n",
        "          final_dic[self.cache[cache_key].main_topic] = (self.cache[cache_key])\n",
        "          continue\n",
        "        # Rate limiting\n",
        "        self.rate_limiter.wait_if_needed()\n",
        "        #Process content\n",
        "        result = self.extract_strucutred_data(dic[\"raw_content\"])\n",
        "        if result:\n",
        "          self.cache[cache_key] = result\n",
        "          final_dic[result.main_topic] = result\n",
        "        else:\n",
        "          print(f\"Extraction failed for {current_title}\")\n",
        "      return final_dic\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Simple rate limiter for API calls.\"\"\"\n",
        "\n",
        "    def __init__(self, requests_per_minute: int):\n",
        "        self.requests_per_minute = requests_per_minute\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait if we're hitting rate limits.\"\"\"\n",
        "        current_time = time.time()\n",
        "        min_interval = 60.0 / self.requests_per_minute\n",
        "\n",
        "        if current_time - self.last_request_time < min_interval:\n",
        "            wait_time = min_interval - (current_time - self.last_request_time)\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n"
      ],
      "metadata": {
        "id": "HHcOjcEsZlbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate optimized processing\n",
        "optimized_processor = OptimizedProcessor(client)\n",
        "print(\"⚡ Optimized processor created with batching and caching\")\n",
        "structured_outputs = optimized_processor.batch_extract(scrapper_result)"
      ],
      "metadata": {
        "id": "r6LZ-AyBmsa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0d91c2-f7c4-4c1c-df28-2d8da1fe1067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Optimized processor created with batching and caching\n",
            "✅ Structured extraction successful!\n",
            "✅ Structured extraction successful!\n",
            "✅ Structured extraction successful!\n",
            "✅ Structured extraction successful!\n",
            "✅ Structured extraction successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate error handling, if no client is parsed\n",
        "optimized_processor_missing_client = OptimizedProcessor()\n",
        "print(\"⚡ Optimized processor created with batching and caching\")\n",
        "structured_outputs_missing_client = optimized_processor_missing_client.batch_extract(scrapper_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23GNbuj0_fMe",
        "outputId": "c3ea75da-834b-41fa-eb4d-e904e6336f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Optimized processor created with batching and caching\n",
            "❌ Extraction failed: 'NoneType' object has no attribute 'chat'\n",
            "Extraction failed for Agentic AI - Wikipedia\n",
            "❌ Extraction failed: 'NoneType' object has no attribute 'chat'\n",
            "Extraction failed for Reinforcement learning - Wikipedia\n",
            "❌ Extraction failed: 'NoneType' object has no attribute 'chat'\n",
            "Extraction failed for Machine learning - Wikipedia\n",
            "❌ Extraction failed: 'NoneType' object has no attribute 'chat'\n",
            "Extraction failed for Deep learning - Wikipedia\n",
            "❌ Extraction failed: 'NoneType' object has no attribute 'chat'\n",
            "Extraction failed for Natural language processing - Wikipedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze structued outputs:\n",
        "The following code shows the output after extraction and what patterns to expect. Remember we have 7 field\n",
        "```\n",
        "main_topic, summary, evolution_timeline, key_innovations, major_contributors, main_techniques, applications\n",
        "```\n",
        "The structured outputs is a dictionary where `main_topic` is used as the key. This seems to be better than crawl4ai's title, at least in Wikipedia case. Instead of giving `\"Agentic AI - Wikipedia\"`, the extracted `main_topic` gives `\"Agentic AI\"`"
      ],
      "metadata": {
        "id": "HzUiogBy_zcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[item for item in structured_outputs.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4zeqcRAn52",
        "outputId": "48a0370f-445f-4d06-b877-538523cf620c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Agentic AI',\n",
              " 'Reinforcement Learning',\n",
              " 'Machine Learning',\n",
              " 'Deep Learning',\n",
              " 'Natural Language Processing']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'summary: {structured_outputs[\"Machine Learning\"].summary}')\n",
        "print(f'evolution_timeline: {structured_outputs[\"Machine Learning\"].evolution_timeline}')\n",
        "print(f'key_innovations: {structured_outputs[\"Machine Learning\"].key_innovations}')\n",
        "print(f'major_contributors: {structured_outputs[\"Machine Learning\"].major_contributors}')\n",
        "print(f'main_techniques: {structured_outputs[\"Machine Learning\"].main_techniques}')\n",
        "print(f'applications: {structured_outputs[\"Machine Learning\"].applications}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TajNmAa-E8CU",
        "outputId": "0ecf7f9d-f317-4149-d625-3b734e0831e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary: Machine learning (ML) is a subfield of artificial intelligence that focuses on creating systems that can learn from and make predictions based on data. It encompasses a variety of techniques and theories that allow computers to improve their performance on tasks through experience.\n",
            "evolution_timeline: • 1959: The term 'machine learning' is coined by Arthur Samuel. \n",
            "• 1990s: Focus shifts from achieving artificial intelligence to practical solvable problems. \n",
            "• 2010s: Deep learning emerges, leveraging neural networks for higher accuracy in various applications.\n",
            "key_innovations: • Development of decision trees. \n",
            "• Introduction of neural networks and backpropagation. \n",
            "• Emergence of support vector machines. \n",
            "• Development of deep learning architectures.\n",
            "major_contributors: • Arthur Samuel, Tom Mitchell, Geoffrey Hinton, Yann LeCun, Andrew Ng, Fei-Fei Li.\n",
            "main_techniques: ['Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Decision trees', 'Support vector machines', 'Neural networks', 'Deep learning', 'Clustering']\n",
            "applications: ['Natural language processing', 'Computer vision', 'Speech recognition', 'Financial fraud detection', 'Healthcare diagnostics', 'Predictive analytics in various fields', 'Recommendation systems', 'Autonomous vehicles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Function calling implementaion\n",
        "The following codes are function calling implementation, `compare_technologies` and `trace_evolution`\n",
        "`tech1`, `tech2` and `technology` are the arguments, later in the schema we bound them to the keys in our structured_outputs"
      ],
      "metadata": {
        "id": "sjlaPJD4CyuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_technologies(tech1:str, tech2:str):\n",
        "  \"\"\"Compare two technologies\"\"\"\n",
        "  tech1_output = structured_outputs[tech1]\n",
        "  tech2_output = structured_outputs[tech2]\n",
        "  if not tech1_output or not tech2_output:\n",
        "    return \"One or both technologies not found.\"\n",
        "  comparison_prompt = f\"\"\"\n",
        "  Compare the following technologies based on their summary, main_techniques ,applications\n",
        "\n",
        "  {tech1_output.main_topic}:\n",
        "  Summary: {tech1_output.summary}\n",
        "  Main Techniques: {tech1_output.main_techniques}\n",
        "  Applications: {tech1_output.applications}\n",
        "\n",
        "  {tech2_output.main_topic}:\n",
        "  Summary: {tech2_output.summary}\n",
        "  Main Techniques: {tech2_output.main_techniques}\n",
        "  Applications: {tech2_output.applications}\n",
        "  \"\"\"\n",
        "  #search first for the topic\n",
        "  return comparison_prompt\n",
        "\n",
        "def trace_evolution(technology:str):\n",
        "  \"\"\"Trace the evolution of a technology\"\"\"\n",
        "  tech_output = structured_outputs[technology]\n",
        "  if not tech_output:\n",
        "    return \"Technology not found.\"\n",
        "  evolution_prompt = f\"\"\"\n",
        "  Trace the evolution of the following technology:\n",
        "\n",
        "  {tech_output.main_topic}:\n",
        "  Summary: {tech_output.summary}\n",
        "  Evolution Timeline: {tech_output.evolution_timeline}\n",
        "  Major Contributors: {tech_output.major_contributors}\n",
        "  Key Innovations: {tech_output.key_innovations}\n",
        "  \"\"\"\n",
        "  return evolution_prompt"
      ],
      "metadata": {
        "id": "59MLmktx-FKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(compare_technologies(\"Agentic AI\", \"Machine Learning\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4GMDTQjK3YA",
        "outputId": "744b89b6-9ed0-467d-ae86-a3de1da040f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Compare the following technologies based on their summary, main_techniques ,applications\n",
            "\n",
            "  Agentic AI:\n",
            "  Summary: Agentic AI is a class of artificial intelligence focused on autonomous systems that can make decisions and perform tasks with or without human intervention, utilizing various AI techniques such as natural language processing and machine learning.\n",
            "  Main Techniques: ['Natural Language Processing', 'Machine Learning', 'Computer Vision', 'Robotic Process Automation']\n",
            "  Applications: ['Software Development', 'Customer Support', 'Cybersecurity', 'Business Intelligence', 'Web Browsing']\n",
            "\n",
            "  Machine Learning:\n",
            "  Summary: Machine learning (ML) is a subfield of artificial intelligence that focuses on creating systems that can learn from and make predictions based on data. It encompasses a variety of techniques and theories that allow computers to improve their performance on tasks through experience.\n",
            "  Main Techniques: ['Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Decision trees', 'Support vector machines', 'Neural networks', 'Deep learning', 'Clustering']\n",
            "  Applications: ['Natural language processing', 'Computer vision', 'Speech recognition', 'Financial fraud detection', 'Healthcare diagnostics', 'Predictive analytics in various fields', 'Recommendation systems', 'Autonomous vehicles']\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compare_technologies(\"Deep Learning\", \"Machine Learning\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9SHdHSYVHIR",
        "outputId": "6f448661-e020-4132-e330-2aaa669ab1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Compare the following technologies based on their summary, main_techniques ,applications\n",
            "\n",
            "  Deep Learning:\n",
            "  Summary: Deep Learning is a subfield of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. It imitates how human brains operate by stacking artificial neurons and training them to process massive amounts of data, often leading to breakthroughs in various domains, particularly in computer vision, speech recognition, and natural language processing.\n",
            "  Main Techniques: ['Neural Networks', 'Convolutional Neural Networks (CNNs)', 'Recurrent Neural Networks (RNNs)', 'Transformers', 'Generative Adversarial Networks (GANs)', 'Deep Belief Networks (DBNs)', 'Autoencoders']\n",
            "  Applications: ['Image recognition', 'Speech recognition', 'Natural language processing', 'Medical image analysis', 'Recommender systems', 'Autonomous vehicles', 'Game playing', 'Financial fraud detection', 'Drug discovery and bioinformatics', 'Climate modeling and weather forecasting']\n",
            "\n",
            "  Machine Learning:\n",
            "  Summary: Machine learning (ML) is a subfield of artificial intelligence that focuses on creating systems that can learn from and make predictions based on data. It encompasses a variety of techniques and theories that allow computers to improve their performance on tasks through experience.\n",
            "  Main Techniques: ['Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Decision trees', 'Support vector machines', 'Neural networks', 'Deep learning', 'Clustering']\n",
            "  Applications: ['Natural language processing', 'Computer vision', 'Speech recognition', 'Financial fraud detection', 'Healthcare diagnostics', 'Predictive analytics in various fields', 'Recommendation systems', 'Autonomous vehicles']\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that `enum` has `valid_technologies`, so as we expand the urls, the number of topics would increase automatically."
      ],
      "metadata": {
        "id": "j1W3m36EDbBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bound technologies to what is in structured_outputs\n",
        "valid_technologies = [item for item in structured_outputs.keys()]\n",
        "\n",
        "#Define function schema for OpenAI\n",
        "compare_technologies_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"compare_technologies\",\n",
        "        \"description\": \"Compare two technologies\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"tech1\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The first technology to compare\",\n",
        "                    \"enum\": valid_technologies\n",
        "                },\n",
        "                \"tech2\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The second technology to compare\",\n",
        "                    \"enum\": valid_technologies\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"tech1\", \"tech2\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "trace_evolution_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"trace_evolution\",\n",
        "        \"description\": \"Trace the historical evolution of a given technology\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"technology\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The technology to trace\",\n",
        "                    \"enum\": valid_technologies\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"technology\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "KakcRQtSMECW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_technologies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OutVPQ90EBry",
        "outputId": "71a6bcac-28e8-4b0e-b7d6-1a557bb6afd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Agentic AI',\n",
              " 'Reinforcement Learning',\n",
              " 'Machine Learning',\n",
              " 'Deep Learning',\n",
              " 'Natural Language Processing']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Integration and Testing"
      ],
      "metadata": {
        "id": "2ey_RHemZTF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_function_calling(natural_queries:List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Demonstrate function calling with natural language weather queries.\n",
        "\n",
        "    This shows the complete flow:\n",
        "    Natural Language → Function Call → API Response → Structured Data → Natural Answer\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "      print(\"❌ OpenAI client not initialized. Exiting.\")\n",
        "      return\n",
        "\n",
        "    results = []\n",
        "    for query in natural_queries:\n",
        "        print(f\"🔍 Query: {query}\")\n",
        "        try:\n",
        "          # Step 1: Send query to OpenAI with function definition\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "\n",
        "                        \"content\": f\"\"\"You are a helpful research assistant. You have access to detailed data for these technologies: {valid_technologies}.\n",
        "                        Use the relevant function to answer the question.\n",
        "                        When users ask about relationships between technologies, use the compare_technologies function.\n",
        "                        When users ask about the history or evolution of a technology, use the trace_evolution function.\n",
        "                        The functions can handle technologies not in the database by providing general analysis.\n",
        "                        Always try to use the appropriate function for comparison or evolution questions.\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": query\n",
        "                    }, # Added comma here\n",
        "                ],\n",
        "                tools=[compare_technologies_schema, trace_evolution_schema], #Available functions\n",
        "                tool_choice=\"auto\",  # Let the model decide when to call functions\n",
        "            )\n",
        "            response_message = response.choices[0].message\n",
        "            # Step 2: Check if the model wants to call a function\n",
        "            if response_message.tool_calls:\n",
        "              tool_call = response_message.tool_calls[0]\n",
        "              function_name = tool_call.function.name\n",
        "              function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "              print(f\"🔧 LLM decided to call: {function_name}({function_args})\")\n",
        "            # Step 3: Execute the function\n",
        "              if function_name == \"compare_technologies\":\n",
        "                result = compare_technologies(**function_args)\n",
        "              elif function_name == \"trace_evolution\":\n",
        "                result = trace_evolution(**function_args)\n",
        "              if result:\n",
        "                try:\n",
        "                  response = client.chat.completions.create(\n",
        "                      model=\"gpt-4o-mini\",\n",
        "                      messages=[\n",
        "                          {\n",
        "                              \"role\": \"system\",\n",
        "                              \"content\": f\"You are a helpful answering assistant. Use the following context {result} to answer the user query. If you dont know the answer, just say you dont know.\"\n",
        "                          },\n",
        "                          {\n",
        "                              \"role\": \"user\",\n",
        "                              \"content\": query\n",
        "                          }, # Added another comma here\n",
        "                      ],\n",
        "                  )\n",
        "                  answer = response.choices[0].message.content\n",
        "                  print(answer)\n",
        "                  results.append(answer)\n",
        "                except Exception as e:\n",
        "                  print(f\"⚠️ Error generating final answer: {e}\")\n",
        "                  results.append(f\"⚠️ Error generating final answer: {e}\")\n",
        "                  continue\n",
        "            else:\n",
        "              print(\"⚠️ No function supports query\")\n",
        "              results.append(\"No function supports query\")\n",
        "              continue\n",
        "        except Exception as e:\n",
        "          print(f\"⚠️ Error processing query: {e}\")\n",
        "          results.append(f\"⚠️ Error generating final answer: {e}\")\n",
        "          continue\n",
        "    return results"
      ],
      "metadata": {
        "id": "Bm7S9Tzz_hiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = demonstrate_function_calling(\n",
        "    [\n",
        "        \"What is the evolution for Reinforcement Learning?\",\n",
        "        \"How is machine learning and deep learning related?\",\n",
        "        \"Are agentic AI related to natural language processing?\",\n",
        "        \"How should I connect reinforcement learning with deep learning?\"\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoEpMJyRA2W",
        "outputId": "177e2c7f-01ad-40ec-8ee6-b9809bfaf474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Query: What is the evolution for Reinforcement Learning?\n",
            "🔧 LLM decided to call: trace_evolution({'technology': 'Reinforcement Learning'})\n",
            "The evolution of Reinforcement Learning (RL) can be traced through several key developments over the years:\n",
            "\n",
            "- **1950s**: Initial concepts of reinforcement learning were developed, primarily in the fields of robotics and control theory.\n",
            "- **1980**: The introduction of Temporal Difference (TD) learning marked a significant advancement in RL, allowing for learning from temporally extended sequences of actions.\n",
            "- **1992**: The Q-learning algorithm was proposed, providing a framework for learning the value of actions in a given state without needing a model of the environment.\n",
            "- **2015**: The Deep Q-Network (DQN) was introduced, showcasing successful applications of reinforcement learning techniques in playing Atari games, utilizing deep learning methods to enhance performance.\n",
            "\n",
            "Throughout this evolution, several major contributors have played significant roles, including Richard S. Sutton, Andrew Barto, David Silver, Volodymyr Mnih, and Demis Hassabis. Key innovations in the field also include Q-learning, Policy Gradient Methods, Deep Q-Networks (DQN), Actor-Critic methods, Proximal Policy Optimization (PPO), and Temporal Difference Learning.\n",
            "🔍 Query: How is machine learning and deep learning related?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Machine Learning', 'tech2': 'Deep Learning'})\n",
            "Machine learning and deep learning are closely related, with deep learning being a subfield of machine learning. \n",
            "\n",
            "Machine learning focuses on creating systems that learn from data and improve their performance over time, using various techniques such as supervised learning, unsupervised learning, reinforcement learning, decision trees, and support vector machines.\n",
            "\n",
            "Deep learning, on the other hand, specifically utilizes multilayered neural networks to perform tasks such as classification and regression. It mimics the way human brains operate by stacking artificial neurons to process large amounts of data, leading to significant advancements in tasks like image recognition, speech recognition, and natural language processing.\n",
            "\n",
            "In summary, all deep learning techniques are part of the broader machine learning field, but not all machine learning techniques are deep learning.\n",
            "🔍 Query: Are agentic AI related to natural language processing?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Agentic AI', 'tech2': 'Natural Language Processing'})\n",
            "Yes, Agentic AI is related to Natural Language Processing (NLP). Agentic AI utilizes various AI techniques, including NLP, to enable autonomous systems to understand and generate human language as part of their decision-making and task execution processes. NLP facilitates communication and interaction between these AI systems and humans, making it a valuable component of Agentic AI applications.\n",
            "🔍 Query: How should I connect reinforcement learning with deep learning?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Reinforcement Learning', 'tech2': 'Deep Learning'})\n",
            "You can connect reinforcement learning with deep learning by utilizing deep learning techniques to enhance the capabilities of reinforcement learning algorithms. This approach is often referred to as Deep Reinforcement Learning (DRL). Here are some ways they can be connected:\n",
            "\n",
            "1. **Function Approximation**: In many reinforcement learning problems, especially those with large state or action spaces, traditional methods like Q-learning may struggle due to the inability to maintain a comprehensive table of values. Deep learning can be used to approximate the value functions or policy functions, allowing RL agents to handle more complex environments. For instance, Deep Q-Networks (DQN) use deep neural networks to approximate Q-values.\n",
            "\n",
            "2. **Feature Extraction**: Deep learning excels at automatically learning useful feature representations from raw data. In reinforcement learning, you can apply deep learning architectures (like Convolutional Neural Networks for image data) to extract relevant features from the input, thereby simplifying the learning task for the RL agent.\n",
            "\n",
            "3. **Policy Representation**: Deep learning allows for the representation of more complex policies through neural networks. Policy Gradient Methods can benefit from using deep neural networks to model the policy function that maps states to actions directly.\n",
            "\n",
            "4. **Handling High-Dimensional Inputs**: In domains like robotics, gaming, and complex simulation environments with high-dimensional input data (such as images), deep learning provides the necessary tools to process and learn from this data effectively.\n",
            "\n",
            "These connections have led to significant advancements in areas such as robotics, game playing (e.g., AlphaGo, OpenAI's Dota 2 agent), and real-world applications where RL and DL are employed together to solve complex decision-making tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error handling for non related questions\n",
        "For questions outside of the scope, No function will support query, it will still perform query for correct questions."
      ],
      "metadata": {
        "id": "EKTaoJhz92Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "WHZ2xK2O8UxM",
        "outputId": "de9dfa40-2b61-4f89-8b4b-79a89fb18b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning and deep learning are closely related, with deep learning being a subfield of machine learning. \\n\\nMachine learning focuses on creating systems that learn from data and improve their performance over time, using various techniques such as supervised learning, unsupervised learning, reinforcement learning, decision trees, and support vector machines.\\n\\nDeep learning, on the other hand, specifically utilizes multilayered neural networks to perform tasks such as classification and regression. It mimics the way human brains operate by stacking artificial neurons to process large amounts of data, leading to significant advancements in tasks like image recognition, speech recognition, and natural language processing.\\n\\nIn summary, all deep learning techniques are part of the broader machine learning field, but not all machine learning techniques are deep learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_results = demonstrate_function_calling(\n",
        "    [\n",
        "        \"What did I have for dinner?\",\n",
        "        \"Is AI good\",\n",
        "        \"How is machine learning and deep learning related?\",\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qcyv2XG8tsa",
        "outputId": "3e844a6c-c41d-48a9-f63f-981b513f14e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Query: What did I have for dinner?\n",
            "⚠️ No function supports query\n",
            "🔍 Query: Is AI good\n",
            "⚠️ No function supports query\n",
            "🔍 Query: How is machine learning and deep learning related?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Machine Learning', 'tech2': 'Deep Learning'})\n",
            "Machine learning and deep learning are closely related concepts in the field of artificial intelligence, but they differ in their scope and techniques. \n",
            "\n",
            "- **Machine Learning** is a broader field that encompasses a variety of techniques and approaches for creating systems that can learn from data and make predictions. It includes methods like supervised learning, unsupervised learning, reinforcement learning, decision trees, support vector machines, neural networks, and clustering.\n",
            "\n",
            "- **Deep Learning**, on the other hand, is a specialized subset of machine learning. It specifically focuses on using multilayered neural networks (also known as deep neural networks) to perform complex tasks such as classification and regression. Deep learning effectively handles high-dimensional data and has led to significant advancements in areas such as computer vision, speech recognition, and natural language processing.\n",
            "\n",
            "In summary, while all deep learning is machine learning, not all machine learning is deep learning. Deep learning uses the more complex architectures of neural networks to achieve high performance on specific tasks, whereas machine learning includes a wider range of algorithms and techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_results[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "K8xtM7SJ-Pub",
        "outputId": "3adbc8f3-0a44-49e4-a729-fd16eb488bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning and deep learning are closely related concepts in the field of artificial intelligence, but they differ in their scope and techniques. \\n\\n- **Machine Learning** is a broader field that encompasses a variety of techniques and approaches for creating systems that can learn from data and make predictions. It includes methods like supervised learning, unsupervised learning, reinforcement learning, decision trees, support vector machines, neural networks, and clustering.\\n\\n- **Deep Learning**, on the other hand, is a specialized subset of machine learning. It specifically focuses on using multilayered neural networks (also known as deep neural networks) to perform complex tasks such as classification and regression. Deep learning effectively handles high-dimensional data and has led to significant advancements in areas such as computer vision, speech recognition, and natural language processing.\\n\\nIn summary, while all deep learning is machine learning, not all machine learning is deep learning. Deep learning uses the more complex architectures of neural networks to achieve high performance on specific tasks, whereas machine learning includes a wider range of algorithms and techniques.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_results[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cIHjSoDc-aDg",
        "outputId": "fe784f43-2ded-4459-c215-a1398a13ed58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No function supports query'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "wf0xh2K9-mI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display of scrapped data\n",
        "Below is the scrapped data, we jsut show one topic, Agentic AI"
      ],
      "metadata": {
        "id": "Z9OUqDGWAEPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in scrapper_result[0].items():\n",
        "  if key != \"raw_content\":\n",
        "    print(f\"{key} : {value}\")\n",
        "  else:\n",
        "    print(f\"{key} : {value[:2000] + \"...\"}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dervhat4AIe0",
        "outputId": "ae362db0-2e0f-4744-d775-b931b4798d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url : https://en.wikipedia.org/wiki/Agentic_AI\n",
            "title : Agentic AI - Wikipedia\n",
            "raw_content : # Agentic AI\n",
            "4 languages\n",
            "  * [Català](https://ca.wikipedia.org/wiki/IA_ag%C3%A8ntica \"IA agèntica – Catalan\")\n",
            "  * [فارسی](https://fa.wikipedia.org/wiki/%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C_%D8%AE%D9%88%D8%AF%D9%85%D8%AE%D8%AA%D8%A7%D8%B1 \"هوش مصنوعی خودمختار – Persian\")\n",
            "  * [한국어](https://ko.wikipedia.org/wiki/%EC%97%90%EC%9D%B4%EC%A0%84%ED%8B%B1_AI \"에이전틱 AI – Korean\")\n",
            "  * [Русский](https://ru.wikipedia.org/wiki/%D0%90%D0%B3%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D0%98%D0%98 \"Агентный ИИ – Russian\")\n",
            "\n",
            "  * [Article](https://en.wikipedia.org/wiki/Agentic_AI \"View the content page \\[alt-c\\]\")\n",
            "  * [Talk](https://en.wikipedia.org/wiki/Talk:Agentic_AI \"Discuss improvements to the content page \\[alt-t\\]\")\n",
            "\n",
            "English\n",
            "  * [Read](https://en.wikipedia.org/wiki/Agentic_AI)\n",
            "  * [Edit](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=edit \"Edit this page \\[alt-e\\]\")\n",
            "  * [View history](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=history \"Past revisions of this page \\[alt-h\\]\")\n",
            "\n",
            "  * [Read](https://en.wikipedia.org/wiki/Agentic_AI)\n",
            "  * [Edit](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=edit \"Edit this page \\[alt-e\\]\")\n",
            "  * [View history](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=history)\n",
            "\n",
            "General \n",
            "  * [What links here](https://en.wikipedia.org/wiki/Special:WhatLinksHere/Agentic_AI \"List of all English Wikipedia pages containing links to this page \\[alt-j\\]\")\n",
            "  * [Related changes](https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Agentic_AI \"Recent changes in pages linked from this page \\[alt-k\\]\")\n",
            "  * [Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard \"Upload files \\[alt-u\\]\")\n",
            "  * [Permanent link](https://en.wikipedia.org/w/index.php?title=Agentic_AI&oldid=1308638935 \"Permanent link to this revision of this page\")\n",
            "  * [Page information](https://en.wikipedia.org/w/index.php?title=Agentic_AI&action=info \"More information about this page\")\n",
            "  * [Cite this page](https://en.wikipedia...\n",
            "success : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured extraction results"
      ],
      "metadata": {
        "id": "bsyCl3sm-_jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in structured_outputs.items():\n",
        "  print(f\"AI subtopic: {key}\")\n",
        "  print(f\"Summary: {value.summary}\")\n",
        "  print(f\"Evolution Timeline: {value.evolution_timeline}\")\n",
        "  print(f\"Key Innovations: {value.key_innovations}\")\n",
        "  print(f\"Major Contributors: {value.major_contributors}\")\n",
        "  print(f\"Main Techniques: {value.main_techniques}\")\n",
        "  print(f\"Applications: {value.applications}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXo6K6Lq-uGD",
        "outputId": "2f08979c-cc07-40b0-8418-9ffd42b360f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI subtopic: Agentic AI\n",
            "Summary: Agentic AI is a class of artificial intelligence focused on autonomous systems that can make decisions and perform tasks with or without human intervention, utilizing various AI techniques such as natural language processing and machine learning.\n",
            "Evolution Timeline: 1998: The term 'agent-based process management system' is first used to describe autonomous agents for business process management.\n",
            "Key Innovations: Introductions of intelligent agents, autonomous systems, and advancements in algorithmic and human-like creative processes.\n",
            "Major Contributors: MIT, TechCrunch, companies involved in automation and software development.\n",
            "Main Techniques: ['Natural Language Processing', 'Machine Learning', 'Computer Vision', 'Robotic Process Automation']\n",
            "Applications: ['Software Development', 'Customer Support', 'Cybersecurity', 'Business Intelligence', 'Web Browsing']\n",
            "\n",
            "\n",
            "AI subtopic: Reinforcement Learning\n",
            "Summary: Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents should take actions in an environment to maximize cumulative rewards. It focuses on decision-making processes that involve balancing exploration (trying new actions) and exploitation (using known beneficial actions).\n",
            "Evolution Timeline: 1950s: Initial concepts of reinforcement learning are developed in robotics and control theory. 1980: Temporal difference learning is introduced. 1992: Q-learning algorithm is proposed. 2015: Deep Q-Network (DQN) demonstrates successful applications in playing Atari games.\n",
            "Key Innovations: Q-learning, Policy Gradient Methods, Deep Q-Networks (DQN), Actor-Critic methods, Proximal Policy Optimization (PPO), and Temporal Difference Learning.\n",
            "Major Contributors: Richard S. Sutton, Andrew Barto, David Silver, Volodymyr Mnih, and Demis Hassabis.\n",
            "Main Techniques: ['Q-learning', 'SARSA (State-Action-Reward-State-Action)', 'Deep Q-Networks (DQN)', 'Policy Gradient Methods', 'Actor-Critic Methods', 'Temporal Difference Learning']\n",
            "Applications: ['Gaming (e.g., AlphaGo)', 'Robotics and Autonomous Control', 'Financial Trading', 'Healthcare Optimization', 'Traffic Control', 'Recommender Systems', 'Natural Language Processing (e.g., Reinforcement Learning from Human Feedback)']\n",
            "\n",
            "\n",
            "AI subtopic: Machine Learning\n",
            "Summary: Machine learning (ML) is a subfield of artificial intelligence that focuses on creating systems that can learn from and make predictions based on data. It encompasses a variety of techniques and theories that allow computers to improve their performance on tasks through experience.\n",
            "Evolution Timeline: • 1959: The term 'machine learning' is coined by Arthur Samuel. \n",
            "• 1990s: Focus shifts from achieving artificial intelligence to practical solvable problems. \n",
            "• 2010s: Deep learning emerges, leveraging neural networks for higher accuracy in various applications.\n",
            "Key Innovations: • Development of decision trees. \n",
            "• Introduction of neural networks and backpropagation. \n",
            "• Emergence of support vector machines. \n",
            "• Development of deep learning architectures.\n",
            "Major Contributors: • Arthur Samuel, Tom Mitchell, Geoffrey Hinton, Yann LeCun, Andrew Ng, Fei-Fei Li.\n",
            "Main Techniques: ['Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Decision trees', 'Support vector machines', 'Neural networks', 'Deep learning', 'Clustering']\n",
            "Applications: ['Natural language processing', 'Computer vision', 'Speech recognition', 'Financial fraud detection', 'Healthcare diagnostics', 'Predictive analytics in various fields', 'Recommendation systems', 'Autonomous vehicles']\n",
            "\n",
            "\n",
            "AI subtopic: Deep Learning\n",
            "Summary: Deep Learning is a subfield of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. It imitates how human brains operate by stacking artificial neurons and training them to process massive amounts of data, often leading to breakthroughs in various domains, particularly in computer vision, speech recognition, and natural language processing.\n",
            "Evolution Timeline: 1943: The first neural network model, the McCulloch-Pitts Neuron, proposed.\n",
            "1958: Frank Rosenblatt introduces the Perceptron, the first model of a neural network capable of learning.\n",
            "1986: Backpropagation becomes popular for training deep networks.\n",
            "2006: Geoffrey Hinton and colleagues revive interest in deep learning through deep belief networks.\n",
            "2012: AlexNet achieves groundbreaking results in the ImageNet competition, marking the start of the deep learning revolution.\n",
            "Key Innovations: Introduction of multilayer perceptrons, backpropagation algorithm, convolutional neural networks (CNNs), recurrent neural networks (RNNs), Long Short-Term Memory (LSTM) units, Generative Adversarial Networks (GANs), and transformers.\n",
            "Major Contributors: Yoshua Bengio, Geoffrey Hinton, Yann LeCun, Alex Krizhevsky, Ian Goodfellow, Andrew Ng, and Demis Hassabis.\n",
            "Main Techniques: ['Neural Networks', 'Convolutional Neural Networks (CNNs)', 'Recurrent Neural Networks (RNNs)', 'Transformers', 'Generative Adversarial Networks (GANs)', 'Deep Belief Networks (DBNs)', 'Autoencoders']\n",
            "Applications: ['Image recognition', 'Speech recognition', 'Natural language processing', 'Medical image analysis', 'Recommender systems', 'Autonomous vehicles', 'Game playing', 'Financial fraud detection', 'Drug discovery and bioinformatics', 'Climate modeling and weather forecasting']\n",
            "\n",
            "\n",
            "AI subtopic: Natural Language Processing\n",
            "Summary: Natural Language Processing (NLP) is a subfield of artificial intelligence focused on the interaction between computers and humans through natural language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
            "Evolution Timeline: 1950s: Roots of NLP with Alan Turing's publication of the Turing test; 1960s: Development of systems like SHRDLU and ELIZA; 1970s: Growth of ontologies and first chatterbots; late 1980s: Shift to statistical NLP, introducing machine learning; 2003: Word n-gram model and the rise of neural networks; 2010s: Popularization of deep learning techniques in NLP tasks culminating in advanced models like BERT and GPT.\n",
            "Key Innovations: The introduction of machine learning algorithms for language processing, development of word embeddings (like Word2Vec), the implementation of neural networks in NLP, and advances in understanding linguistic structures through deep learning techniques.\n",
            "Major Contributors: Alan Turing, Joseph Weizenbaum, Tomáš Mikolov, Yoshua Bengio, Geoffrey Hinton, Stanford Natural Language Processing Group.\n",
            "Main Techniques: ['Tokenization', 'Part-of-speech tagging', 'Named entity recognition', 'Sentiment analysis', 'Machine translation', 'Deep learning methods', 'Transformers', 'Word embeddings']\n",
            "Applications: ['Chatbots and virtual assistants', 'Text summarization', 'Speech recognition', 'Machine translation', 'Sentiment analysis in social media', 'Information extraction from texts', 'Text classification for document organization', 'Automated essay scoring']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample function calling converstaions"
      ],
      "metadata": {
        "id": "LyRdZ4uiA5QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = demonstrate_function_calling(\n",
        "    [\n",
        "        \"What is the evolution for Reinforcement Learning?\",\n",
        "        \"How is machine learning and deep learning related?\",\n",
        "        \"Are agentic AI related to natural language processing?\",\n",
        "        \"How should I connect reinforcement learning with deep learning?\"\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MPMDF58A4fT",
        "outputId": "f632e854-c13a-43cb-aaed-848ac5cad81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Query: What is the evolution for Reinforcement Learning?\n",
            "🔧 LLM decided to call: trace_evolution({'technology': 'Reinforcement Learning'})\n",
            "The evolution of Reinforcement Learning (RL) can be traced through several key developments over the decades:\n",
            "\n",
            "- **1950s**: Initial concepts of reinforcement learning are developed, particularly in the context of robotics and control theory. This period laid the groundwork for understanding how agents can learn to make decisions based on their interactions with environments.\n",
            "\n",
            "- **1980**: The concept of Temporal Difference Learning is introduced, which allows agents to learn optimal policies by estimating the value of states based on predictions of future rewards.\n",
            "\n",
            "- **1992**: The Q-learning algorithm is proposed, offering a model-free approach for agents to learn the value of actions using a value function.\n",
            "\n",
            "- **2015**: The Deep Q-Network (DQN) is developed, demonstrating successful applications of reinforcement learning in playing complex video games, particularly Atari games. This marked a significant leap in the use of deep learning techniques in RL.\n",
            "\n",
            "Throughout this evolution, several major contributors have played pivotal roles, including Richard S. Sutton, Andrew Barto, David Silver, Volodymyr Mnih, and Demis Hassabis. Key innovations also emerged, such as Q-learning, Policy Gradient Methods, Deep Q-Networks (DQN), Actor-Critic methods, Proximal Policy Optimization (PPO), and Temporal Difference Learning.\n",
            "🔍 Query: How is machine learning and deep learning related?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Machine Learning', 'tech2': 'Deep Learning'})\n",
            "Machine learning and deep learning are closely related, as deep learning is a subset of machine learning. Here’s how they are connected:\n",
            "\n",
            "1. **Hierarchy**: Machine learning is a broader field that encompasses various algorithms and techniques that enable systems to learn from data. Deep learning, on the other hand, specifically focuses on techniques that utilize neural networks with multiple layers (hence \"deep\") to analyze and learn from large amounts of data.\n",
            "\n",
            "2. **Techniques**: While machine learning includes a wide range of techniques (such as supervised learning, unsupervised learning, reinforcement learning, decision trees, etc.), deep learning specifically employs multilayered neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to model complex patterns and representations in data.\n",
            "\n",
            "3. **Applications**: Both machine learning and deep learning are used in similar applications, especially in fields like natural language processing, speech recognition, and autonomous vehicles. However, deep learning often excels in areas that require processing large datasets and capturing high-level abstractions, such as image and speech data.\n",
            "\n",
            "In summary, deep learning is a specialized area within machine learning that focuses on deep neural networks to perform tasks more effectively, particularly in handling large-scale and complex datasets.\n",
            "🔍 Query: Are agentic AI related to natural language processing?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Agentic AI', 'tech2': 'Natural Language Processing'})\n",
            "Yes, agentic AI is related to natural language processing (NLP). Agentic AI utilizes various AI techniques, including NLP, to enable autonomous systems to make decisions and perform tasks. NLP allows these systems to interact with humans using natural language, enhancing their ability to understand and respond to user queries in a meaningful way.\n",
            "🔍 Query: How should I connect reinforcement learning with deep learning?\n",
            "🔧 LLM decided to call: compare_technologies({'tech1': 'Reinforcement Learning', 'tech2': 'Deep Learning'})\n",
            "Reinforcement Learning (RL) and Deep Learning (DL) can be connected through the integration of deep neural networks into the RL framework. This combination is commonly referred to as Deep Reinforcement Learning (DRL). Here are a few ways to connect the two:\n",
            "\n",
            "1. **Function Approximation**: Deep Learning models, especially neural networks, can be used to approximate the value functions or policy functions in RL. This allows agents to handle high-dimensional state spaces that would be infeasible with traditional RL methods.\n",
            "\n",
            "2. **Deep Q-Networks (DQN)**: One of the prominent examples of combining RL and DL is through Deep Q-Networks, where a deep neural network is used to approximate the Q-values for different actions, enabling the agent to make more informed decisions in complex environments.\n",
            "\n",
            "3. **Policy Gradient Methods**: In policy-based approaches, deep neural networks can be used to parameterize a policy, allowing the agent to learn directly the action distributions over states rather than relying on value functions.\n",
            "\n",
            "4. **Actor-Critic Methods**: These methods utilize two neural networks—an actor that decides which action to take and a critic that assesses how good the action taken was. This setup benefits from the strengths of both value-based and policy-based methods.\n",
            "\n",
            "5. **Applications**: The synergy between RL and DL has led to advancements in various applications such as gaming (e.g., AlphaGo), robotics, autonomous control, and natural language processing, where complex decision-making and perception are required.\n",
            "\n",
            "In summary, connecting reinforcement learning with deep learning involves leveraging deep neural networks to improve the efficiency and capability of RL algorithms in handling large and complex environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for reply in results:\n",
        "  print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYSy3a8xBTJv",
        "outputId": "cb46f0af-4576-437b-ebc9-02d797e8ee22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evolution of Reinforcement Learning (RL) can be traced through several key developments over the decades:\n",
            "\n",
            "- **1950s**: Initial concepts of reinforcement learning are developed, particularly in the context of robotics and control theory. This period laid the groundwork for understanding how agents can learn to make decisions based on their interactions with environments.\n",
            "\n",
            "- **1980**: The concept of Temporal Difference Learning is introduced, which allows agents to learn optimal policies by estimating the value of states based on predictions of future rewards.\n",
            "\n",
            "- **1992**: The Q-learning algorithm is proposed, offering a model-free approach for agents to learn the value of actions using a value function.\n",
            "\n",
            "- **2015**: The Deep Q-Network (DQN) is developed, demonstrating successful applications of reinforcement learning in playing complex video games, particularly Atari games. This marked a significant leap in the use of deep learning techniques in RL.\n",
            "\n",
            "Throughout this evolution, several major contributors have played pivotal roles, including Richard S. Sutton, Andrew Barto, David Silver, Volodymyr Mnih, and Demis Hassabis. Key innovations also emerged, such as Q-learning, Policy Gradient Methods, Deep Q-Networks (DQN), Actor-Critic methods, Proximal Policy Optimization (PPO), and Temporal Difference Learning.\n",
            "Machine learning and deep learning are closely related, as deep learning is a subset of machine learning. Here’s how they are connected:\n",
            "\n",
            "1. **Hierarchy**: Machine learning is a broader field that encompasses various algorithms and techniques that enable systems to learn from data. Deep learning, on the other hand, specifically focuses on techniques that utilize neural networks with multiple layers (hence \"deep\") to analyze and learn from large amounts of data.\n",
            "\n",
            "2. **Techniques**: While machine learning includes a wide range of techniques (such as supervised learning, unsupervised learning, reinforcement learning, decision trees, etc.), deep learning specifically employs multilayered neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to model complex patterns and representations in data.\n",
            "\n",
            "3. **Applications**: Both machine learning and deep learning are used in similar applications, especially in fields like natural language processing, speech recognition, and autonomous vehicles. However, deep learning often excels in areas that require processing large datasets and capturing high-level abstractions, such as image and speech data.\n",
            "\n",
            "In summary, deep learning is a specialized area within machine learning that focuses on deep neural networks to perform tasks more effectively, particularly in handling large-scale and complex datasets.\n",
            "Yes, agentic AI is related to natural language processing (NLP). Agentic AI utilizes various AI techniques, including NLP, to enable autonomous systems to make decisions and perform tasks. NLP allows these systems to interact with humans using natural language, enhancing their ability to understand and respond to user queries in a meaningful way.\n",
            "Reinforcement Learning (RL) and Deep Learning (DL) can be connected through the integration of deep neural networks into the RL framework. This combination is commonly referred to as Deep Reinforcement Learning (DRL). Here are a few ways to connect the two:\n",
            "\n",
            "1. **Function Approximation**: Deep Learning models, especially neural networks, can be used to approximate the value functions or policy functions in RL. This allows agents to handle high-dimensional state spaces that would be infeasible with traditional RL methods.\n",
            "\n",
            "2. **Deep Q-Networks (DQN)**: One of the prominent examples of combining RL and DL is through Deep Q-Networks, where a deep neural network is used to approximate the Q-values for different actions, enabling the agent to make more informed decisions in complex environments.\n",
            "\n",
            "3. **Policy Gradient Methods**: In policy-based approaches, deep neural networks can be used to parameterize a policy, allowing the agent to learn directly the action distributions over states rather than relying on value functions.\n",
            "\n",
            "4. **Actor-Critic Methods**: These methods utilize two neural networks—an actor that decides which action to take and a critic that assesses how good the action taken was. This setup benefits from the strengths of both value-based and policy-based methods.\n",
            "\n",
            "5. **Applications**: The synergy between RL and DL has led to advancements in various applications such as gaming (e.g., AlphaGo), robotics, autonomous control, and natural language processing, where complex decision-making and perception are required.\n",
            "\n",
            "In summary, connecting reinforcement learning with deep learning involves leveraging deep neural networks to improve the efficiency and capability of RL algorithms in handling large and complex environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Insights and Reflection\n",
        "## Key Insights:\n",
        "\n",
        "###Interconnected Nature of AI Tehcnologies:\n",
        "Although AI subtopics are often introduced individually, these methods are constantly being used in conjuction to create more powerful products. For example, q learning combined with deep neural netowrk results in Deep Q-Networks(DQN), Agentic AI incorperates techniques like natural language processing. This interconnectedness shows that modern AI solutions are rarely built on a single technique but rather represent convergent evolution of multiple AI approaches.\n",
        "\n",
        "###Educational Value of Structured Analysis:\n",
        "From a student's perspective, this system can easily create connections without requiring students to read through entire Wikipedia articles.The structured format provides quick access to evolution timelines, key innovations, and major contributors, giving students both breadth and depth in understanding how different AI fields developed and relate to each other.\n",
        "\n",
        "###Historical Context Matters:\n",
        "The evolution analysis revealed that understanding the timeline of AI development helps appreciate current capabilities. For instance, seeing how Reinforcement Learning evolved from 1950s psychology principles to 2020s sophisticated algorithms provides crucial context for understanding why certain approaches work and others don't.\n",
        "\n",
        "## Design decisions and code structure explanations\n",
        "###Data Storage Strategy\n",
        "The biggest architectural decision was how to store and query structured outputs. Initially, I considered implementing a query analysis system to extract main topics from a given list, which would have required additional API calls for topic identification. However, I ultimately chose to store structured outputs in a dictionary with the main topic as the key, using enum constraints to bound function calls to available topics. This design eliminated one API call to OpenAI while maintaining clean, predictable data access patterns. <br>\n",
        "\n",
        "###Prompt Engineering Strategies\n",
        "I had to do some prompt engineering to make it more likely for methods to be used as I realized a generic \"Choose the relevant function\" results in no functions being called for some of the questions. <br>\n",
        "\n",
        "###Error Handling\n",
        "The system includes robust error handling that continues processing queries even when individual queries fail, and provides fallback responses when functions aren't called or data isn't available. It also prints which part of the code has errors.\n",
        "\n",
        "## Challenges faced and solutions implemented\n",
        "Throughout the assignment, I struggled with deciding the general flow of a relatively complex task. For example, should chunking be used, how to store the outputs, what format to use.\n",
        "\n",
        "Solution: I adopted an iterative and wishful thinking approach during the desings. Suppose I already have two correct and perfect functions to analyze a given technology, how would I showcase the demo. This really helped with the details and I had to keep rethinking and rewriting certain parts of the code to fit. Intially structured_outputs was just a list, but if I need to query, a dictionary would make much more sense. Then test cases where I expect certain output and how do enhance prompt engineering to make sure the right functions are called.\n",
        "\n",
        "## Meaningful Patterns and Connections Uncovered\n",
        "###Overlap of technologies\n",
        "The analysis revales how different AI fields often overlap and require similar techniques and overlap in scope. Technologies like Agentic AI demonstrate how modern AI applications integrate techniques from multiple traditional AI subfields, suggesting that future AI development will be increasingly interdisciplinary. This assitance helps to connect the dots easily, and is a fruit for thought as I learn more about my masters\n",
        "\n",
        "##What you learned from this assignment\n",
        "###Technical Skills:\n",
        "- Scraping with crawl4ai\n",
        "- Structured outputs with LLM\n",
        "- function calling, function schemas and prompt engineering\n",
        "- error handling and strategies for robust systems\n",
        "- designing an AI system\n",
        "###Domain Knowledge:\n",
        "- Gained deeper appreciation for the interconnected nature of AI technologies\n",
        "- Understanding how historical context enriches current AI discussions\n",
        "- Recognition that effective AI education need to highlight relationships between concepts, not just individual topic explanations\n",
        "\n"
      ],
      "metadata": {
        "id": "jY3hazeAL_N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI Tool Declaration\n",
        "I used GPT-5 and Claude Sonnet 4 to generate ideas, seek advice for unsure code, debug and consolidate my ideas. I am responsible for the content and quality of the submitted work."
      ],
      "metadata": {
        "id": "tr4NcNWkXRMC"
      }
    }
  ]
}